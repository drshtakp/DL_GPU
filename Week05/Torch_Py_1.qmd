---
title: "Pytorch Demo 1"
format:
  html:
    code-fold: true
jupyter: python3
---

From "Inside Deep Learning" by Edward Raff

https://github.com/EdwardRaff/Inside-Deep-Learning

## R-Python Toolkits if Run in RStudio

`library(reticulate)`
Gilbreth `use_python("/depot/gdsp/apps/MLPy/bin/python3")`
Local `use_python("/Users/wen-wen/opt/miniconda3/envs/dl/bin/python3")`
`py_config()`
`repl_python()`

## Jupyter Kernel to Use if Run in Jupyter Notebook
Gilbreth: `learning/conda-2020.11-py38-gpu` or `MLPy-py3.8.5`

## Packages and Environment

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from tqdm.autonotebook import tqdm
import pandas as pd
```

```{python}
# Uncomment the magic command in Jupyter Notebook to enable inline display of Matplotlib
# %matplotlib inline
from IPython.display import set_matplotlib_formats
set_matplotlib_formats('png', 'pdf')
```

```{python}
import torch
print(f'Torch: {torch.__version__}\nCUDA: {torch.cuda.is_available()}')
```

CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use NVIDIA GPUs (Graphics Processing Units) for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units).

```{python}
torch_scalar = torch.tensor(3.14)
torch_vector = torch.tensor([1, 2, 3, 4])
torch_matrix = torch.tensor([[1, 2,],
                             [3, 4,],
                             [5, 6,], 
                             [7, 8,]])

torch_tensor3d = torch.tensor([
                            [
                            [ 1,  2,  3], 
                            [ 4,  5,  6],
                            ],
                            [
                            [ 7,  8,  9], 
                            [10, 11, 12],
                            ],
                            [
                            [13, 14, 15], 
                            [16, 17, 18],
                            ],
                            [
                            [19, 20, 21], 
                            [22, 23, 24],
                            ]
                              ])
```


```{python}
print(torch_scalar.shape)
print(torch_vector.shape)
print(torch_matrix.shape)
print(torch_tensor3d.shape)
```

```{python}
x_np = np.random.random((4,4))
print(x_np)
```

```{python}
x_pt = torch.tensor(x_np)
print(x_pt)
```

```{python}
print(x_np.dtype, x_pt.dtype)
```

```{python}
x_np = np.asarray(x_np, dtype=np.float32)
x_pt = torch.tensor(x_np, dtype=torch.float32)
print(x_np.dtype, x_pt.dtype)
```

```{python}
b_np = (x_np > 0.5)
print(b_np)
print(b_np.dtype)
```

```{python}
b_pt = (x_pt > 0.5)
print(b_pt)
print(b_pt.dtype)
```

```{python}
np.sum(x_np)
```

```{python}
torch.sum(x_pt)
```

```{python}
np.transpose(x_np)
```

```{python}
torch.transpose(x_pt, 0, 1)
```

```{python}
print(torch.transpose(torch_tensor3d, 0, 2).shape)
```

The chunk below measures how long (in seconds) it takes to perform matrix multiplication on a 2048x2048 matrix using the CPU, repeating the operation 100 times. The argument `globals=globals()` ensures that the `timeit` function has access to the global namespace, allowing it to recognize and use the previously defined `x` matrix.

```{python}
import timeit
x = torch.rand(2**11, 2**11)
time_cpu = timeit.timeit("x@x", globals=globals(), number=100)
time_cpu
```

```{python}
# Don't uncomment the cuda device setting if no GPU
# print("Is CUDA available? :", torch.cuda.is_available())
# device = torch.device("cuda")
device = torch.device("cpu")

x = x.to(device)
time_gpu = timeit.timeit("x@x", globals=globals(), number=100)
```

```{python}
def moveTo(obj, device):
    """
    obj: the python object to move to a device, or to move its contents to a device
    device: the compute device to move objects to
    """
    if isinstance(obj, list):
        return [moveTo(x, device) for x in obj]
    elif isinstance(obj, tuple):
        return tuple(moveTo(list(obj), device))
    elif isinstance(obj, set):
        return set(moveTo(list(obj), device))
    elif isinstance(obj, dict):
        to_ret = dict()
        for key, value in obj.items():
            to_ret[moveTo(key, device)] = moveTo(value, device)
        return to_ret
    elif hasattr(obj, "to"):
        return obj.to(device)
    else:
        return obj
    
some_tensors = [torch.tensor(1), torch.tensor(2)]
print(some_tensors)
print(moveTo(some_tensors, device))
```

```{python}
def f(x):
    return torch.pow((x-2.0), 2)

x_axis_vals = np.linspace(-7,9,100) 
y_axis_vals = f(torch.tensor(x_axis_vals)).numpy()

sns.lineplot(x=x_axis_vals, y=y_axis_vals, label='$f(x)=(x-2)^2$')
plt.show()
```

```{python}
def fP(x): #Defining the derivative of f(x) manually
    return 2*x-4

y_axis_vals_p = fP(torch.tensor(x_axis_vals)).numpy()

#First, lets draw a black line at 0, so that we can easily tell if something is positive or negative
sns.lineplot(x=x_axis_vals, y=[0.0]*len(x_axis_vals), label="0", color='black')
sns.lineplot(x=x_axis_vals, y=y_axis_vals, label='$f(x) = (x-2)^2$')
sns.lineplot(x=x_axis_vals, y=y_axis_vals_p, label="$f'(x)=2 x - 4$")
plt.show()

```

```{python}
x = torch.tensor([-3.5], requires_grad=True)
print(x.grad)
```

```{python}
value = f(x)
print(value)
```

```{python}
value.backward()
print(x.grad)
```

```{python}
x = torch.tensor([-3.5], requires_grad=True)

x_cur = x.clone()
x_prev = x_cur*100 #Make the initial "previous" solution larger
epsilon = 1e-5
eta = 0.1

while torch.linalg.norm(x_cur-x_prev) > epsilon:
    x_prev = x_cur.clone() #We need to make a clone here so that x_prev and x_cur don't point to the same object
    
    #Compute our function, gradient, and update
    value = f(x)
    value.backward()
    x.data -= eta * x.grad
    x.grad.zero_() #We need to zero out the old gradient, as py-torch will not do that for us
    
    #What are we currently now?
    x_cur = x.data
    
print(x_cur)
```

```{python}
x_param = torch.nn.Parameter(torch.tensor([-3.5]), requires_grad=True)
```

```{python}
optimizer = torch.optim.SGD([x_param], lr=eta)
```

```{python}
#| tags: [remove_output]
for epoch in range(60):
    optimizer.zero_grad() #x.grad.zero_()
    loss_incurred  = f(x_param)
    loss_incurred.backward()
    optimizer.step() #x.data -= eta * x.grad
print(x_param.data)
```

```{python}
from torch.utils.data import Dataset
from sklearn.datasets import fetch_openml

# Load data from https://www.openml.org/d/554
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
print(X.shape)
```

```{python}
class SimpleDataset(Dataset):
        
    def __init__(self, X, y):
        super(SimpleDataset, self).__init__()
        self.X = X
        self.y = y
        
    def __getitem__(self, index):
    # Check if X and y are pandas data structures and handle indexing accordingly
        X_data = self.X.iloc[index, :] if hasattr(self.X, 'iloc') else self.X[index, :]
        y_data = self.y.iloc[index] if hasattr(self.y, 'iloc') else self.y[index]

        inputs = torch.tensor(X_data, dtype=torch.float32)
        targets = torch.tensor(int(y_data), dtype=torch.int64)
    
        return inputs, targets
      
    def __len__(self):
        return self.X.shape[0]
#Now we can make a PyTorch dataset 
dataset = SimpleDataset(X, y)
```

```{python}
print("Length: ", len(dataset))
example, label = dataset[0]
print("Features: ", example.shape) #Will return 784
print("Label of index 0: ", label)
```

```{python}
#| max_h: 0.3
#| max_w: 0.9
plt.imshow(example.reshape((28,28)))
plt.show()
```

```{python}
train_size = int(len(dataset)*0.8)
test_size = len(dataset)-train_size

train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, test_size))
print("{} examples for training and {} for testing".format(len(train_dataset), len(test_dataset)))
```
